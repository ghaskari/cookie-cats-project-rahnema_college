# -*- coding: utf-8 -*-
"""Rahnema college | Ghazal Askari.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12KyqpHWA1_d6ZXjhZ8XTW27DpFqmC7KG
"""

import pandas as pd
from google.colab import files
import matplotlib.pyplot as plt
import numpy as np
from operator import lt,gt
import seaborn as sns

!pip install pandas-profiling==2.7.1
from pandas_profiling import ProfileReport

df = pd.read_csv (r'Online shopping.csv')
df

"""**1.Describe**"""

df.shape

df.describe()

df.info()

!pip install pandas==1.2.5

profile = ProfileReport(df)

profile.to_file('df.html')

"""**2.Missing Values** """

df.isna().sum()

def nan_randomly(column__):

  numbers = column__.isna().sum()
  values_ = list(set( column__.dropna())) 

  nan_index = column__.loc[pd.isna(column__)].index
  choose = np.random.choice(values_,numbers)

  column__.loc[nan_index] = choose

  return column__

def nan_mode(COLUMN):
  
  number = COLUMN.isna().sum()

  value =  COLUMN.value_counts().idxmax() 

  COLUMN.fillna(value,inplace=True)

  return COLUMN

"""**Random** Variable"""

miss = df.copy()
miss = miss.apply(nan_randomly,axis=0)
miss.isna().sum()

"""**Mode value**"""

modemiss = df.copy()
modemiss = modemiss.apply(nan_mode,axis=0)
modemiss.isna().sum()

"""**3. Dataset Graph**"""

sns.set()

dataForHistogram = df.drop(['Weekend', 'Revenue'], axis=1)
dataForHistogram['Weekend'] = df['Weekend'].astype(float)
dataForHistogram['Revenue'] = df['Revenue'].astype(float)
dataForHistogram.hist(figsize=(20, 15))
plt.show()

"""**4.categorical features**"""

miss = pd.get_dummies(df,columns=df.columns[[10,15]])

modemiss = pd.get_dummies(df,columns=df.columns[[10,15]])

"""**5.Correlation**"""

corrdata1=miss.corr()
corrdata1

plt.figure(figsize=(50,50))
sns.heatmap(corrdata1,annot=True,)
plt.show()

"""**6.Testing Models**"""

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.impute import SimpleImputer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import OneHotEncoder

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) 

chosenFeatures = df[['PageValues', 'ExitRates', 'Region', 'Month', 'Revenue', 'ProductRelated', 'Weekend', 'Informational_Duration']].copy()
chosenFeatures['PageValueOverProductRelated'] = df['PageValues'] / df['ProductRelated']

for train_index, test_index in split.split(chosenFeatures, chosenFeatures["Revenue"]):
  strat_train_set = chosenFeatures.loc[train_index] 
  strat_test_set = chosenFeatures.loc[test_index]

data = strat_train_set.drop('Revenue', axis = 1)
data_labels = strat_train_set['Revenue'].copy()
test = strat_test_set.drop('Revenue', axis = 1)
test_labels = strat_test_set['Revenue'].copy()

class RandomImputer(BaseEstimator, TransformerMixin):

    def __init__(self):
      self.values = {}

    def fit(self, X, Y=None):
        for col in X.columns:
          randomValue = random.choice([x for x in X[col].unique() if str(x) != 'nan'])
          self.values[col] = randomValue
        return self 
    
    def transform(self, X, Y=None):
        out = X.copy()
        out = out.fillna(value = self.values)
        return out.values

cat_attributes = ['Weekend', 'Region', 'Month']
data_num = data.drop(cat_attributes, axis = 1)
num_attributes = list(data_num)

imputer = SimpleImputer(strategy="most_frequent")
num_pipeline = Pipeline([ 
                         ('imputer', imputer), 
                         ('std_scaler', StandardScaler()) 
                         ])

cat_pipeline = Pipeline([ 
                         ('imputer', imputer), 
                         ('onehot', OneHotEncoder()) 
                         ])

preprocess = ColumnTransformer([
                                ('num', num_pipeline, num_attributes),
                                ('cat', cat_pipeline, cat_attributes) 
                                ])

model = SVC(kernel='rbf')
full_pipeline = Pipeline([
                          ('preprocess', preprocess),
                          ('model', model),
                          ])

full_pipeline.fit(data, data_labels)
print("Model Train Accuracy: " + str(full_pipeline.score(data, data_labels)))